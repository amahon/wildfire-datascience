{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in a Blaze - Part 1\n",
    "\n",
    "Tying up my time at [Mapillary](https://mapillary.com) in mid January, I decided that instead of diving directly back into a job hunt, that I would take a bit of time to hone existing skills and develop some new. Ever since extracting and visualizing sentiment data from New York Times comments as an undergraduate in design school, I've been attacted to data analysis as a means to gain and share a greater understanding of the world. With some time on my hands and a passion for writing software in Python, I decided to dive in heads first. This is a first this series of blog posts where I'll catalog my self-education, share some  of what I build, and hopefully provide an opportunity for others with an interest to learn alongside.\n",
    "\n",
    "### Self Education\n",
    "\n",
    "Thus far, my eduction has been self driven on [Kaggle](http://kaggle.com/) - there's a ton of knowledge there! While my experience diving into the domain has been pretty humbling -- pretty much everyone around me knows so much more, it's also been enlightening and energizing -- the possibilities are boundless.\n",
    "\n",
    "Thinking about next steps, I plan to:\n",
    "- complete an end to end machine learning project and publish my progress on this blog and on Kaggle\n",
    "- complete fast.ai [Deep Learning Part 1](http://course.fast.ai/)\n",
    "- build a deep learning computer\n",
    "- compete in at least one Kaggle competetion, hopefully joining a team to boost my learning\n",
    "\n",
    "### E2E: Predicting the cause of Wildfires in the United States\n",
    "\n",
    "<img src='http://www.futurity.org/wp/wp-content/uploads/2017/10/wildfire-on-mountain_1600.jpg' />\n",
    "\n",
    "Surveying the datasets available to work with on Kaggle, I was immediately attracted to a dataset describing [1.88M US Wildfires](https://www.kaggle.com/rtatman/188-million-us-wildfires) over 15 years [originally published](https://www.fs.usda.gov/rds/archive/Product/RDS-2013-0009.4/) by the US Forest Service. Between a geospatial component, high topical relevance, and personal interest in the subject, I decided that I'd focus my first end to end project on predicting the cause of a Fire given information available when the fire began. Given the limited information available in the data set, it's highly likely that integrating additional data such as historical weather or land use, will be essential to building a strong model.\n",
    "\n",
    "This first blog post outlines covers the initial steps of preparing an environment and data to begin working with it.\n",
    "\n",
    "### Changes\n",
    "\n",
    "Since the project is still moving, I expect the content here to change, stay tuned here.\n",
    "\n",
    "**2017/1/30** - initial post\n",
    "\n",
    "### Some notes on Notebooks/Jupyter/Kaggle\n",
    "\n",
    "This post, and work, was completed in a Jupyter notebook running in a docker container. While my process has been roughly cataloged [here](https://www.andrewmahon.info/blog/docker-compose-data-science), throughout the project, the container has been modified a bit to update existing tools and provide new tools. The Dockerfile and associated notebooks can be found on [github](https://github.com/amahon/wildfire-datascience), and I'll write a blog post about it sooner or later. I also suspect that the code, verbatim, won't run on kaggle -- there are some geo related libraries that likley need to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "\n",
    "The first thing that we need to do is prepare our working environment by importing libraries and performing some configuration incantations.\n",
    "\n",
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:49:38.805828Z",
     "start_time": "2018-02-08T17:49:37.663487Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "import geopandas as gpd\n",
    "import graphviz\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import palettable\n",
    "import pandas as pd\n",
    "import pandas.tools.plotting as pdplot\n",
    "import pprint\n",
    "import pyproj\n",
    "from rasterstats import zonal_stats\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "from shapely.ops import transform\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import subprocess\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from sqlalchemy.types import String, JSON\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "from tqdm import tqdm , tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:49:39.281781Z",
     "start_time": "2018-02-08T17:49:39.259687Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (15, 15)\n",
    "mpl.rcParams['agg.path.chunksize'] = 100000\n",
    "\n",
    "qual_colormap = palettable.matplotlib.Inferno_20\n",
    "quant_colormap = palettable.matplotlib.Inferno_20_r\n",
    "\n",
    "mpl.rcParams['image.cmap'] = qual_colormap.mpl_colormap\n",
    "sns.set(rc={'figure.figsize':(15, 15)})\n",
    "sns.set_palette(qual_colormap.mpl_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Now that we have our libraries in place, we'll load in our data and have a look at it. Since our data comes in a SQLite format, we can use Pandas' [`read_sql_query`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html) functionality to build a data frame. Since we're not yet acquainted with the contents of our data, we'll load all provided columns and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:49:55.630410Z",
     "start_time": "2018-02-08T17:49:41.014722Z"
    }
   },
   "outputs": [],
   "source": [
    "input_filename = '/data/188-million-us-wildfires/src/FPA_FOD_20170508.sqlite'\n",
    "conn = sqlite3.connect(input_filename)\n",
    "\n",
    "query = '''\n",
    "    SELECT\n",
    "        NWCG_REPORTING_AGENCY,\n",
    "        NWCG_REPORTING_UNIT_ID,\n",
    "        NWCG_REPORTING_UNIT_NAME,\n",
    "        FIRE_NAME,\n",
    "        COMPLEX_NAME,\n",
    "        FIRE_YEAR,\n",
    "        DISCOVERY_DATE,\n",
    "        DISCOVERY_DOY,\n",
    "        DISCOVERY_TIME,\n",
    "        STAT_CAUSE_CODE,\n",
    "        STAT_CAUSE_DESCR,\n",
    "        CONT_DATE,\n",
    "        CONT_DOY,\n",
    "        CONT_TIME,\n",
    "        FIRE_SIZE,\n",
    "        FIRE_SIZE_CLASS,\n",
    "        LATITUDE,\n",
    "        LONGITUDE,\n",
    "        OWNER_CODE,\n",
    "        OWNER_DESCR,\n",
    "        STATE,\n",
    "        COUNTY\n",
    "    FROM\n",
    "        Fires;\n",
    "'''\n",
    "\n",
    "raw_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Raw Data\n",
    "\n",
    "Now that our data is loaded, let's give it a very high level look and start to develop an understanding of what we're working with.\n",
    "\n",
    "### Info\n",
    "Let's have a look at our column names and the type of data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:49:55.644855Z",
     "start_time": "2018-02-08T17:49:55.632621Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "Let's see how many values are missing in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:49:57.257022Z",
     "start_time": "2018-02-08T17:49:55.646439Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample\n",
    "Let's look at some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:49:57.401392Z",
     "start_time": "2018-02-08T17:49:57.258685Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:04.015208Z",
     "start_time": "2018-02-08T17:49:57.405600Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Considering that our investigation is around predicting the cause of a wildfire from the time that it was discovered, I've made the following initial observations about the data:\n",
    "- **STAT_CAUSE_CODE** and **STAT_CAUSE_DESCR** are related and represent the value that we are trying to predict. Before training, we'll drop **STAT_CAUSE_DESCR** in favor of the numerical value of **STAT_CAUSE_CODE**.\n",
    "- **OWNER_CODE** and **OWNER_DESCR** are related and describe the owner of the property where the fire was discovered. This is an interesting value because it represents the land management and usage of a particular peice of land. These will be interesting in our investigation. Before training, we'll drop **OWNER_DESCR** in favor of the numerical value of **OWNER_CODE**.\n",
    "- **DISCOVERY_DATE**, **DISCOVERY_DOY**, **DISCOVERY_TIME** describe the time that a fire was discovered. **DISCOVERY_DOY** is most interesting to our investigation due to it's relation to climate and usage patterns of a particular peice of land. **DISCOVERY_TIME** may be interesting due, but also might be too fine grained, additionally, it's missing values - let's drop it for now. **DISCOVERY_DATE** is [TKTK]\n",
    "- **LATITUDE**, and **LONGITUDE** are both very interesting due to their very high relationship to land cover, land use, and climate - all three big factors in wildfire creation.\n",
    "- **STATE** and  both categorically describe the location of a fire. **STATE** might be interesting due to it's relation in land use patterns. **STATE** also might prove to be a useful generalization of the more specific **LATITUDE**, and **LONGITUDE*.\n",
    "- **COUNTY**, while potentially interesting, has too many missing values. If we want to more closely explore categorial location data, we can add it via a geocoding process in the data engineering process.\n",
    "- A number of columns contain information about how a fire was addressed and not about what caused the fire. Lets' ignore the following columns for now: **NWCG_REPORTING_AGENCY**, **NWCG_REPORTING_UNIT_ID**, **NWCG_REPORTING_UNIT_NAME**, **FIRE_NAME**, **FIRE_COMPLEX**, **CONT_DATE**, **CONT_DOY**, **CONT_TIME**, **FIRE_SIZE**, **FIRE_SIZE_CLASS**\n",
    "\n",
    "This leaves us with the following interesting fields:\n",
    "- **STAT_CAUSE_CODE**\n",
    "- **STAT_CAUSE_DESCR** [for EDA]\n",
    "- **OWNER_CODE**\n",
    "- **OWNER_DESCR**\n",
    "- **DISCOVERY_DOY**\n",
    "- **LATITUDE**\n",
    "- **LONGITUDE**\n",
    "- **STATE**\n",
    "\n",
    "Some things we can keep in our back pocket for future exploration:\n",
    "- look harder at **DISCOVERY_TIME**\n",
    "- look harder at **DISCOVERY_DATE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Readable Mappings\n",
    "Before we drop our human readable columns, let's create a set of mappings that we can use to associate numberical categories back to human readable categories. We'll use these later..\n",
    "\n",
    "Map Cause Code to Cause Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:04.260623Z",
     "start_time": "2018-02-08T17:50:04.017177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STAT_CAUSE_CODE\n",
       "1.0             Lightning\n",
       "2.0         Equipment Use\n",
       "3.0               Smoking\n",
       "4.0              Campfire\n",
       "5.0        Debris Burning\n",
       "6.0              Railroad\n",
       "7.0                 Arson\n",
       "8.0              Children\n",
       "9.0         Miscellaneous\n",
       "10.0            Fireworks\n",
       "11.0            Powerline\n",
       "12.0            Structure\n",
       "13.0    Missing/Undefined\n",
       "Name: STAT_CAUSE_DESCR, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_cause_mapping = raw_df \\\n",
    "    .groupby(['STAT_CAUSE_DESCR', 'STAT_CAUSE_CODE']) \\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .drop(0, axis=1)\\\n",
    "    .set_index('STAT_CAUSE_CODE')\\\n",
    "    .sort_index()['STAT_CAUSE_DESCR']\n",
    "stat_cause_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map Owner Code to Owner Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:05.546413Z",
     "start_time": "2018-02-08T17:50:05.322225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OWNER_CODE\n",
       "0.0                   FOREIGN\n",
       "1.0                       BLM\n",
       "2.0                       BIA\n",
       "3.0                       NPS\n",
       "4.0                       FWS\n",
       "5.0                      USFS\n",
       "6.0             OTHER FEDERAL\n",
       "7.0                     STATE\n",
       "8.0                   PRIVATE\n",
       "9.0                    TRIBAL\n",
       "10.0                      BOR\n",
       "11.0                   COUNTY\n",
       "12.0          MUNICIPAL/LOCAL\n",
       "13.0         STATE OR PRIVATE\n",
       "14.0    MISSING/NOT SPECIFIED\n",
       "15.0        UNDEFINED FEDERAL\n",
       "Name: OWNER_DESCR, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owner_code_mapping = raw_df \\\n",
    "    .groupby(['OWNER_DESCR', 'OWNER_CODE']) \\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .drop(0, axis=1)\\\n",
    "    .set_index('OWNER_CODE')\\\n",
    "    .sort_index()['OWNER_DESCR']\n",
    "owner_code_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip Data\n",
    "\n",
    "Let's create a new dataframe that contains only the fields that we're interested in. This will reduce memory usage and help keep things tidy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:07.926000Z",
     "start_time": "2018-02-08T17:50:07.629530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1880465 entries, 0 to 1880464\n",
      "Data columns (total 8 columns):\n",
      "STAT_CAUSE_CODE     float64\n",
      "STAT_CAUSE_DESCR    object\n",
      "OWNER_CODE          float64\n",
      "OWNER_DESCR         object\n",
      "DISCOVERY_DOY       int64\n",
      "LATITUDE            float64\n",
      "LONGITUDE           float64\n",
      "STATE               object\n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 114.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = raw_df.loc[:,[\n",
    "    'STAT_CAUSE_CODE',\n",
    "    'STAT_CAUSE_DESCR',\n",
    "    'OWNER_CODE',\n",
    "    'OWNER_DESCR',\n",
    "    'DISCOVERY_DOY',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    'STATE'\n",
    "]].copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "We need to split the data into a train set and a test set. The train set will be used to build our model, and the test set will be used to evaluate the model.\n",
    "\n",
    "We will use sklearn's [`model_selection.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split our dataframe into two.\n",
    "\n",
    "Last, we will create a convienience `_df` that allows us to access the union of the test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:20.307090Z",
     "start_time": "2018-02-08T17:50:18.582433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>\n",
       "    Number of Training Rows: 1410348<br />\n",
       "    Number of Test Rows: 470117\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__train_df, __test_df = model_selection.train_test_split(df)\n",
    "\n",
    "fires_df = {\n",
    "    \"train\": __train_df.copy(),\n",
    "    \"test\": __test_df.copy()\n",
    "}\n",
    "\n",
    "display(HTML('''\n",
    "<p>\n",
    "    Number of Training Rows: {}<br />\n",
    "    Number of Test Rows: {}\n",
    "'''.format(fires_df[\"train\"].shape[0], fires_df[\"test\"].shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (for now)\n",
    "\n",
    "Now that we've loaded our data, given it a high level look, cleaned it up a bit, and split it into a train and test set, we're in a good position to begin some Exploratory Data Analysis. Keep an eye here for subsequent posts that will get into:\n",
    "\n",
    "- EDA\n",
    "- Data Engineering\n",
    "- More EDA! (Including geovisualization)\n",
    "- Model Training and Evaulation\n",
    "- Ensembling\n",
    "\n",
    "I hope this was helpful and/or entertaining so far. Since I'm pretty new to this, I'm sure there are, or will be errors - if you see any, please [drop me a line](mailto:andrewmahon@fastmail.com) -- I'd love to fix them right away!\n",
    "\n",
    "Until next time,\n",
    "\n",
    "Andrew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# END BLOG 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Post 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin Data\n",
    "\n",
    "Ref: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html  https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject Data\n",
    "\n",
    "The latitude and longitude values in our input dataset are projected in the [NAD83 coordinate system](https://en.wikipedia.org/wiki/North_American_Datum). \n",
    "\n",
    "Ref: http://jswhit.github.io/pyproj/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Data\n",
    "\n",
    "Some of our data sources may not contain values for Alaska, Hawaii, or Puerto Rico. To simplify the problem set a bit, let's remove data from those states from both our train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:26.604411Z",
     "start_time": "2018-02-08T17:50:24.221006Z"
    }
   },
   "outputs": [],
   "source": [
    "for key, dataframe in fires_df.items():\n",
    "    drop_index = fires_df[key][\n",
    "        (fires_df[key].STATE == \"AK\") |\n",
    "        (fires_df[key].STATE == \"PR\") |\n",
    "        (fires_df[key].STATE == \"HI\")].index\n",
    "    fires_df[key].drop(drop_index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also drop data with a Miscellaneous or Missing cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:41:46.923145Z",
     "start_time": "2018-02-08T21:41:45.774790Z"
    }
   },
   "outputs": [],
   "source": [
    "for key, dataframe in fires_df.items():\n",
    "    drop_index = fires_df[key][\n",
    "        (fires_df[key].STAT_CAUSE_CODE == 9.0) |\n",
    "        (fires_df[key].STAT_CAUSE_CODE == 13.0)].index\n",
    "    fires_df[key].drop(drop_index, inplace=True)\n",
    "    \n",
    "stat_cause_mapping.drop([9.0, 13.0], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data [0] - Exploratory Data Analysis\n",
    "\n",
    "1. Causes\n",
    "1. Week of Year [todo]\n",
    "1. Week of Year and Cause\n",
    "2. Owner\n",
    "3. Owner and Cause\n",
    "4. State\n",
    "5. State and Cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causes\n",
    "\n",
    "Let's explore the causes of wildfires represented in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:28:00.728489Z",
     "start_time": "2018-02-08T21:27:59.819679Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts_by_cause = fires_df[\"train\"].groupby('STAT_CAUSE_DESCR')\\\n",
    "    .size()\\\n",
    "    .sort_values(ascending=False)\n",
    "counts_by_cause_pcts = counts_by_cause.apply(lambda x: 100 * x / float(counts_by_cause.sum()))\n",
    "\n",
    "ax = sns.barplot(counts_by_cause.index, counts_by_cause.values)\n",
    "ax.set_xticklabels(labels=counts_by_cause.index, rotation=90)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    height = p.get_height()\n",
    "    width = p.get_width()\n",
    "    ax.text(\n",
    "        p.get_x()+(width/2.),\n",
    "        height + 1000,\n",
    "        '{:1.2f}%'.format(counts_by_cause_pcts[i]),\n",
    "        ha=\"center\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:31.125530Z",
     "start_time": "2018-02-08T17:50:30.582546Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_by_doy = fires_df[\"train\"].groupby('DISCOVERY_DOY').size()\n",
    "ax = count_by_doy.plot()\n",
    "ax.set_xlim(0,367)\n",
    "ax.set_ylim(0,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of Year and Cause\n",
    "\n",
    "Hypothesis: [TKTK]\n",
    "\n",
    "Process: Add 'DISCOVERY_WEEK' column to table. Note that we end up with 53 weeks as a result of Leap Years. I also added one to 1 index the list of weeks to better adhere with common understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process: Create a piviot table that relates `STAT_CAUSE_DESCR` to `DISCOVERY_WEEK`. Plot that using a seaborne heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:33.462211Z",
     "start_time": "2018-02-08T17:50:32.755328Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cause_by_doy = fires_df[\"train\"].groupby(['DISCOVERY_DOY', 'STAT_CAUSE_DESCR'])\\\n",
    "    .size()\\\n",
    "    .unstack()\n",
    "causes = list(cause_by_doy.columns.values)\n",
    "cause_by_doy['Total'] = cause_by_doy.sum(axis=1)\n",
    "cause_by_doy_proportional = pd.DataFrame()\n",
    "for cause in causes:\n",
    "    cause_by_doy_proportional[cause] = cause_by_doy[[cause, 'Total']].apply(lambda x: x[cause]/x['Total'], axis=1)\n",
    "cause_by_doy = cause_by_doy.drop('Total', axis=1)\n",
    "cause_by_doy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:34.323025Z",
     "start_time": "2018-02-08T17:50:33.498831Z"
    }
   },
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "ax = cause_by_doy.plot.area()\n",
    "ax.set_xlim(0,367)\n",
    "ax.set_ylim(0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:35.052135Z",
     "start_time": "2018-02-08T17:50:34.324906Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    cause_by_doy,\n",
    "    cbar_kws={'shrink':.9 },\n",
    "    annot=False,\n",
    "    cmap=quant_colormap.mpl_colormap\n",
    ")\n",
    "for i, label in enumerate(ax.yaxis.get_ticklabels()):\n",
    "    label.set_visible(False)\n",
    "    if i % 7 == 0:\n",
    "        label.set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:35.881489Z",
     "start_time": "2018-02-08T17:50:35.053650Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    cause_by_doy_proportional,\n",
    "    cbar_kws={'shrink':.9 }, \n",
    "    annot=False,\n",
    "    cmap=quant_colormap.mpl_colormap\n",
    ")\n",
    "for i, label in enumerate(ax.yaxis.get_ticklabels()):\n",
    "    label.set_visible(False)\n",
    "    if i % 7 == 0:\n",
    "        label.set_visible(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: [TKTK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:39.633351Z",
     "start_time": "2018-02-08T17:50:38.533064Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts_by_owner = fires_df[\"train\"].groupby('OWNER_DESCR')\\\n",
    "    .size()\\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "ax = sns.barplot(counts_by_owner.index, counts_by_owner.values)\n",
    "labels = ax.set_xticklabels(labels=counts_by_owner.index, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cause and Owner\n",
    "\n",
    "Add 'DISCOVERY_WEEK' column to table. Note that we end up with 53 weeks as a result of Leap Years. I also added one to 1 index the list of weeks to better adhere with common understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:46.610260Z",
     "start_time": "2018-02-08T17:50:44.813572Z"
    }
   },
   "outputs": [],
   "source": [
    "cause_by_week = fires_df[\"train\"].groupby(['OWNER_DESCR', 'STAT_CAUSE_DESCR'])\\\n",
    "    .size()\\\n",
    "    .unstack()\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    cause_by_week,\n",
    "    cbar_kws={'shrink':.9 }, \n",
    "    annot=False,\n",
    "    cmap='inferno_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:50:47.802013Z",
     "start_time": "2018-02-08T17:50:46.613272Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts_by_state = fires_df[\"train\"].groupby('STATE')\\\n",
    "    .size()\\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "ax = sns.barplot(counts_by_state.index, counts_by_state.values)\n",
    "labels = ax.set_xticklabels(labels=counts_by_state.index, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State, Geographic\n",
    "\n",
    "Load our State outlines, join in some abbreviations.\n",
    "\n",
    "Outlines sourced from: http://eric.clst.org/tech/usgeojson/\n",
    "\n",
    "#### Create States Dataframe containing Border Geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:58:07.860148Z",
     "start_time": "2018-02-08T17:58:07.694969Z"
    }
   },
   "outputs": [],
   "source": [
    "state_outlines_path = '/data/188-million-us-wildfires/src/gz_2010_us_040_00_500k.json'\n",
    "state_outlines_df = gpd.read_file(state_outlines_path).set_index(\"NAME\")\n",
    "state_outlines_df.drop(['Alaska', 'Hawaii', 'Puerto Rico'], inplace=True)\n",
    "\n",
    "state_codes_path = '/data/188-million-us-wildfires/src/state_codes.json'\n",
    "state_codes_df = pd.read_json(state_codes_path, orient='records').set_index('name')\n",
    "state_codes_df.drop(['Alaska', 'Hawaii', 'Puerto Rico'], inplace=True)\n",
    "\n",
    "states = state_outlines_df.join(state_codes_df).set_index('alpha-2')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:58:22.381068Z",
     "start_time": "2018-02-08T17:58:21.927967Z"
    }
   },
   "outputs": [],
   "source": [
    "states.join(counts_by_state.to_frame().rename(columns={0:'count'}))\\\n",
    "    .plot(column='count', cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:58:37.035974Z",
     "start_time": "2018-02-08T17:58:35.540240Z"
    }
   },
   "outputs": [],
   "source": [
    "cause_by_state = fires_df[\"train\"].groupby(['STATE', 'STAT_CAUSE_DESCR'])\\\n",
    "    .size()\\\n",
    "    .unstack()\n",
    "causes = list(cause_by_state.columns.values)\n",
    "cause_by_state['Total'] = cause_by_state.sum(axis=1)\n",
    "cause_by_state_proportional = pd.DataFrame()\n",
    "for cause in causes:\n",
    "    cause_by_state_proportional[cause] = cause_by_state[[cause, 'Total']].apply(lambda x: x[cause]/x['Total'], axis=1)\n",
    "cause_by_state = cause_by_state.drop('Total', axis=1)\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    cause_by_state,\n",
    "    cbar_kws={'shrink':.9 }, \n",
    "    annot=False,\n",
    "    cmap='inferno_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Coorelation\n",
    "\n",
    "Ref: https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:58:42.033468Z",
     "start_time": "2018-02-08T17:58:41.587913Z"
    }
   },
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    #colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    colormap = quant_colormap.mpl_colormap\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        square=True,\n",
    "        cmap = colormap,\n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,\n",
    "        vmax=1.0,\n",
    "        linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(fires_df[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# END BLOG 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify Data\n",
    "\n",
    "A couple fields in our reduced set are still non-numeric, in particular the **STATE** field is still text - let's convert this to a numeric value.\n",
    "\n",
    "Ref: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:58:51.256975Z",
     "start_time": "2018-02-08T17:58:46.762985Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "for key, dataframe in fires_df.items():\n",
    "    dataframe.loc[:,'STATE_CODE'] = label_encoder.fit_transform(dataframe['STATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "fill in fields with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapely Geometry\n",
    "\n",
    "[ed: not sure if i will do this]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T17:58:58.674958Z",
     "start_time": "2018-02-08T17:58:58.672167Z"
    }
   },
   "outputs": [],
   "source": [
    "# geometry = [shapely.geometry.Point(xy) for xy in zip(df.LONGITUDE, df.LATITUDE)]\n",
    "# df.drop(['LONGITUDE', 'LATITUDE'], axis=1, inplace=True)\n",
    "# crs = {'init': 'epsg:4269'}\n",
    "# gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "# del df\n",
    "# gdf = gdf.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "# print(gdf.info())\n",
    "# gdf.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovery Week of Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:31:03.655778Z",
     "start_time": "2018-02-08T21:31:03.115244Z"
    }
   },
   "outputs": [],
   "source": [
    " for key, dataframe in fires_df.items():\n",
    "    dataframe['DISCOVERY_WEEK'] = dataframe['DISCOVERY_DOY']\\\n",
    "        .apply(lambda x: math.floor(x/7) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Regions\n",
    "\n",
    "\n",
    "\n",
    "Ref: https://www.ncdc.noaa.gov/monitoring-references/maps/us-climate-regions.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:31:05.497987Z",
     "start_time": "2018-02-08T21:31:04.087544Z"
    }
   },
   "outputs": [],
   "source": [
    "climate_regions = {\n",
    "    \"northwest\": [\"WA\",\"OR\",\"ID\"],\n",
    "    \"west\": [\"CA\",\"NV\"],\n",
    "    \"southwest\": [\"UT\",\"CO\",\"AZ\",\"NM\"],\n",
    "    \"northern_rockies\": [\"MT\",\"ND\",\"SD\",\"WY\",\"NE\"],\n",
    "    \"upper_midwest\": [\"KS\",\"OK\",\"TX\",\"AR\",\"LA\",\"MS\"],\n",
    "    \"south\": [\"MN\",\"WI\",\"MI\",\"IA\"],\n",
    "    \"ohio_valley\": [\"MO\",\"IL\",\"IN\",\"OH\",\"WV\",\"KY\",\"TN\"],\n",
    "    \"southeast\": [\"VA\",\"NC\",\"SC\",\"GA\",\"AL\",\"FL\"],\n",
    "    \"northeast\": [\"ME\",\"NH\",\"VT\",\"NY\",\"PA\",\"MA\",\"RI\",\"CT\",\"NJ\",\"DE\",\"MD\",\"DC\"],\n",
    "    \"alaska\": [\"AK\",],\n",
    "    \"hawaii\": [\"HI\"],\n",
    "    \"puerto_rico\": [\"PR\"]\n",
    "}\n",
    "\n",
    "state_region_mapping = {}\n",
    "for region, region_states in climate_regions.items():\n",
    "    for state in region_states:\n",
    "        state_region_mapping[state] = region\n",
    "        \n",
    "for key, dataframe in fires_df.items():\n",
    "    dataframe['CLIMATE_REGION'] = dataframe['STATE']\\\n",
    "        .apply(lambda x: state_region_mapping[x])\n",
    "        \n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "for key, dataframe in fires_df.items():\n",
    "    dataframe['CLIMATE_REGION_CODE'] = label_encoder.fit_transform(dataframe['CLIMATE_REGION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3 Bins\n",
    "\n",
    "Ref:\n",
    "\n",
    "https://github.com/uber/h3\n",
    "\n",
    "https://github.com/uber/h3/blob/master/docs/doxyfiles/restable.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcluate Bins for `fires_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:30:44.144011Z",
     "start_time": "2018-02-08T21:28:33.790032Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bins for fires_df[\"train\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5ce3f6f6fc4658b6771d15a54f6f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1410348), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>OWNER_CODE</th>\n",
       "      <th>OWNER_DESCR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>H3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>684488</th>\n",
       "      <td>13.0</td>\n",
       "      <td>Missing/Undefined</td>\n",
       "      <td>8.0</td>\n",
       "      <td>PRIVATE</td>\n",
       "      <td>162</td>\n",
       "      <td>45.200000</td>\n",
       "      <td>-68.600000</td>\n",
       "      <td>ME</td>\n",
       "      <td>832b1efffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175687</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Arson</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>360</td>\n",
       "      <td>31.088169</td>\n",
       "      <td>-90.513067</td>\n",
       "      <td>MS</td>\n",
       "      <td>834442fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386143</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Equipment Use</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>333</td>\n",
       "      <td>36.278890</td>\n",
       "      <td>-91.843310</td>\n",
       "      <td>AR</td>\n",
       "      <td>832658fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36318</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>USFS</td>\n",
       "      <td>186</td>\n",
       "      <td>35.114167</td>\n",
       "      <td>-108.092778</td>\n",
       "      <td>NM</td>\n",
       "      <td>8348ddfffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785446</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>90</td>\n",
       "      <td>38.013748</td>\n",
       "      <td>-91.589930</td>\n",
       "      <td>MO</td>\n",
       "      <td>832642fffffffff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         STAT_CAUSE_CODE   STAT_CAUSE_DESCR  OWNER_CODE  \\\n",
       "684488              13.0  Missing/Undefined         8.0   \n",
       "1175687              7.0              Arson        14.0   \n",
       "386143               2.0      Equipment Use        14.0   \n",
       "36318                1.0          Lightning         5.0   \n",
       "785446               5.0     Debris Burning        14.0   \n",
       "\n",
       "                   OWNER_DESCR  DISCOVERY_DOY   LATITUDE   LONGITUDE STATE  \\\n",
       "684488                 PRIVATE            162  45.200000  -68.600000    ME   \n",
       "1175687  MISSING/NOT SPECIFIED            360  31.088169  -90.513067    MS   \n",
       "386143   MISSING/NOT SPECIFIED            333  36.278890  -91.843310    AR   \n",
       "36318                     USFS            186  35.114167 -108.092778    NM   \n",
       "785446   MISSING/NOT SPECIFIED             90  38.013748  -91.589930    MO   \n",
       "\n",
       "                      H3  \n",
       "684488   832b1efffffffff  \n",
       "1175687  834442fffffffff  \n",
       "386143   832658fffffffff  \n",
       "36318    8348ddfffffffff  \n",
       "785446   832642fffffffff  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bins for fires_df[\"test\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ebb6d423e4415c94d3bf817557097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=470117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>OWNER_CODE</th>\n",
       "      <th>OWNER_DESCR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>H3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1745370</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>126</td>\n",
       "      <td>40.769200</td>\n",
       "      <td>-73.25340</td>\n",
       "      <td>NY</td>\n",
       "      <td>832a10fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633135</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>166</td>\n",
       "      <td>31.368480</td>\n",
       "      <td>-96.34267</td>\n",
       "      <td>TX</td>\n",
       "      <td>834469fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058282</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>59</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>-83.41670</td>\n",
       "      <td>NC</td>\n",
       "      <td>8344cafffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596239</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Children</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>341</td>\n",
       "      <td>35.080000</td>\n",
       "      <td>-86.50333</td>\n",
       "      <td>TN</td>\n",
       "      <td>832649fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92879</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>USFS</td>\n",
       "      <td>239</td>\n",
       "      <td>44.041667</td>\n",
       "      <td>-121.92500</td>\n",
       "      <td>OR</td>\n",
       "      <td>8328a9fffffffff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         STAT_CAUSE_CODE STAT_CAUSE_DESCR  OWNER_CODE            OWNER_DESCR  \\\n",
       "1745370              3.0          Smoking        14.0  MISSING/NOT SPECIFIED   \n",
       "633135               5.0   Debris Burning        14.0  MISSING/NOT SPECIFIED   \n",
       "1058282              5.0   Debris Burning        14.0  MISSING/NOT SPECIFIED   \n",
       "596239               8.0         Children        14.0  MISSING/NOT SPECIFIED   \n",
       "92879                1.0        Lightning         5.0                   USFS   \n",
       "\n",
       "         DISCOVERY_DOY   LATITUDE  LONGITUDE STATE               H3  \n",
       "1745370            126  40.769200  -73.25340    NY  832a10fffffffff  \n",
       "633135             166  31.368480  -96.34267    TX  834469fffffffff  \n",
       "1058282             59  35.200000  -83.41670    NC  8344cafffffffff  \n",
       "596239             341  35.080000  -86.50333    TN  832649fffffffff  \n",
       "92879              239  44.041667 -121.92500    OR  8328a9fffffffff  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def h3_for_chunk(chunk, precision):\n",
    "    lat_lon_lines = \"\\n\".join([\n",
    "        \"{} {}\".format(row['LATITUDE'], row['LONGITUDE']) for index,row in chunk.iterrows()\n",
    "    ])\n",
    "    h3 = subprocess.run(\n",
    "        ['/tools/h3/bin/geoToH3', str(precision)],\n",
    "        input=str.encode(lat_lon_lines),\n",
    "        stdout=subprocess.PIPE).stdout.decode('utf-8').splitlines()\n",
    "    return pd.DataFrame({\n",
    "        \"H3\": h3\n",
    "    }, index=chunk.index)\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq.iloc[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "for key in fires_df:\n",
    "    print('Calculating bins for fires_df[\"{}\"]'.format(key))\n",
    "    h3_df = pd.DataFrame()\n",
    "    with tqdm_notebook(total=fires_df[key].shape[0]) as pbar:\n",
    "        for chunk in chunker(fires_df[key], 10000):\n",
    "            h3_df = h3_df.append(h3_for_chunk(chunk, 3))\n",
    "            pbar.update(chunk.shape[0])    \n",
    "    fires_df[key].drop('H3', 1, inplace=True, errors='ignore')\n",
    "    fires_df[key] = fires_df[key].merge(\n",
    "        h3_df,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        copy=False\n",
    "    )\n",
    "    display(fires_df[key].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:02:23.614359Z",
     "start_time": "2018-02-08T18:02:23.313180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 872 entries, 0 to 871\n",
      "Data columns (total 1 columns):\n",
      "H3    872 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 6.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>832a14fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>832830fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>834889fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>830d56fffffffff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>8326a2fffffffff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  H3\n",
       "595  832a14fffffffff\n",
       "465  832830fffffffff\n",
       "800  834889fffffffff\n",
       "122  830d56fffffffff\n",
       "292  8326a2fffffffff"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = gpd.GeoDataFrame({\n",
    "    'H3': np.unique(np.concatenate((fires_df[\"train\"].H3.unique(), fires_df[\"test\"].H3.unique())))\n",
    "})\n",
    "\n",
    "print(bins.info())\n",
    "bins.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:02:29.439796Z",
     "start_time": "2018-02-08T18:02:27.549843Z"
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins.to_sql(\n",
    "    'bins_0',\n",
    "    engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype={\n",
    "        'H3': String()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revive [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:02:30.351243Z",
     "start_time": "2018-02-08T18:02:30.282922Z"
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins = pd.read_sql(\n",
    "    \"SELECT * FROM bins_0;\",\n",
    "    engine\n",
    ")\n",
    "\n",
    "print(bins.info())\n",
    "bins.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 872 entries, 0 to 871\n",
      "Data columns (total 2 columns):\n",
      "H3          872 non-null object\n",
      "GEOMETRY    872 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H3</th>\n",
       "      <th>GEOMETRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>832988fffffffff</td>\n",
       "      <td>POLYGON ((-118.119011315 38.428511966, -117.71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>83275cfffffffff</td>\n",
       "      <td>POLYGON ((-89.24983803200001 44.754421112, -89...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>8328a8fffffffff</td>\n",
       "      <td>POLYGON ((-119.510979372 43.203253133, -119.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>8344a8fffffffff</td>\n",
       "      <td>POLYGON ((-80.40714699 27.095512737, -79.95771...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>830d59fffffffff</td>\n",
       "      <td>POLYGON ((-144.489378995 64.54236029499999, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>83275dfffffffff</td>\n",
       "      <td>POLYGON ((-88.151586372 43.960885169, -88.7921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>835d14fffffffff</td>\n",
       "      <td>POLYGON ((-156.478385701 21.691033499, -156.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>830c33fffffffff</td>\n",
       "      <td>POLYGON ((-163.941065047 66.024292784, -164.71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>832b18fffffffff</td>\n",
       "      <td>POLYGON ((-67.50096063799998 45.169864573, -68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>8328f0fffffffff</td>\n",
       "      <td>POLYGON ((-122.20136939 45.078437767, -121.800...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  H3                                           GEOMETRY\n",
       "549  832988fffffffff  POLYGON ((-118.119011315 38.428511966, -117.71...\n",
       "398  83275cfffffffff  POLYGON ((-89.24983803200001 44.754421112, -89...\n",
       "507  8328a8fffffffff  POLYGON ((-119.510979372 43.203253133, -119.09...\n",
       "735  8344a8fffffffff  POLYGON ((-80.40714699 27.095512737, -79.95771...\n",
       "124  830d59fffffffff  POLYGON ((-144.489378995 64.54236029499999, -1...\n",
       "399  83275dfffffffff  POLYGON ((-88.151586372 43.960885169, -88.7921...\n",
       "870  835d14fffffffff  POLYGON ((-156.478385701 21.691033499, -156.69...\n",
       "41   830c33fffffffff  POLYGON ((-163.941065047 66.024292784, -164.71...\n",
       "670  832b18fffffffff  POLYGON ((-67.50096063799998 45.169864573, -68...\n",
       "532  8328f0fffffffff  POLYGON ((-122.20136939 45.078437767, -121.800..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def h3_bounds_for_bin(h3_bin):\n",
    "    h3_bounds = subprocess.run(\n",
    "        ['/tools/h3/bin/h3ToGeoBoundary'],\n",
    "        input=str.encode(h3_bin),\n",
    "        stdout=subprocess.PIPE).stdout.decode('utf-8').splitlines()\n",
    "    coords = []\n",
    "    for coord in h3_bounds[2:-1]:\n",
    "        lon_lat = coord.lstrip().split(\" \")\n",
    "        coords.append(((float(lon_lat[1]) - 360.), float(lon_lat[0])))\n",
    "    return shapely.geometry.Polygon(coords)\n",
    "\n",
    "bins['GEOMETRY'] = bins['H3'].apply(h3_bounds_for_bin)\n",
    "\n",
    "print(bins.info())\n",
    "bins.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins['GEOM'] = bins['GEOMETRY'].apply(lambda x: WKTElement(x.wkt))\n",
    "bins.drop('GEOMETRY', 1, inplace=True)\n",
    "\n",
    "bins.to_sql(\n",
    "    'bins_1',\n",
    "    engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype={\n",
    "        'H3': String(),\n",
    "        'GEOM': Geometry('POLYGON')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revive [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:02:38.693704Z",
     "start_time": "2018-02-08T18:02:38.160491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 27099 entries, 0 to 27098\n",
      "Data columns (total 2 columns):\n",
      "H3          27099 non-null object\n",
      "GEOMETRY    27099 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 423.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H3</th>\n",
       "      <th>GEOMETRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>8526252bfffffff</td>\n",
       "      <td>POLYGON ((266.031157912 45.848451921, 265.9448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>85264247fffffff</td>\n",
       "      <td>POLYGON ((268.65528951 37.85176973, 268.728115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15121</th>\n",
       "      <td>852982d7fffffff</td>\n",
       "      <td>POLYGON ((245.423497009 36.924328707, 245.4836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21396</th>\n",
       "      <td>854452bbfffffff</td>\n",
       "      <td>POLYGON ((272.731250984 30.81425055, 272.79935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>8526e0c7fffffff</td>\n",
       "      <td>POLYGON ((262.835267296 38.6668419, 262.907581...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7460</th>\n",
       "      <td>8526dcabfffffff</td>\n",
       "      <td>POLYGON ((260.178242566 34.25216047, 260.24673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>85268a1bfffffff</td>\n",
       "      <td>POLYGON ((253.879548139 37.738385068, 253.9469...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18789</th>\n",
       "      <td>852aa213fffffff</td>\n",
       "      <td>POLYGON ((281.031904016 41.745900817, 280.9318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>85266b87fffffff</td>\n",
       "      <td>POLYGON ((273.900083699 37.170819482, 273.9722...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16833</th>\n",
       "      <td>8529b18ffffffff</td>\n",
       "      <td>POLYGON ((245.046176498 34.821674532, 245.1048...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    H3                                           GEOMETRY\n",
       "1719   8526252bfffffff  POLYGON ((266.031157912 45.848451921, 265.9448...\n",
       "2446   85264247fffffff  POLYGON ((268.65528951 37.85176973, 268.728115...\n",
       "15121  852982d7fffffff  POLYGON ((245.423497009 36.924328707, 245.4836...\n",
       "21396  854452bbfffffff  POLYGON ((272.731250984 30.81425055, 272.79935...\n",
       "7606   8526e0c7fffffff  POLYGON ((262.835267296 38.6668419, 262.907581...\n",
       "7460   8526dcabfffffff  POLYGON ((260.178242566 34.25216047, 260.24673...\n",
       "4599   85268a1bfffffff  POLYGON ((253.879548139 37.738385068, 253.9469...\n",
       "18789  852aa213fffffff  POLYGON ((281.031904016 41.745900817, 280.9318...\n",
       "3818   85266b87fffffff  POLYGON ((273.900083699 37.170819482, 273.9722...\n",
       "16833  8529b18ffffffff  POLYGON ((245.046176498 34.821674532, 245.1048..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT * FROM bins_1;\",\n",
    "    engine,\n",
    "    geom_col='GEOM'\n",
    ")\n",
    "bins.rename(mapper={'GEOM': 'GEOMETRY'}, axis=1, inplace=True)\n",
    "\n",
    "print(bins.info())\n",
    "bins.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Cover Classification\n",
    "Add land cover classification to bins DF.\n",
    "\n",
    "A big flaw to the current method is the \n",
    "<img src=\"https://www.mrlc.gov/images/NLCD06_conus_lg.gif\" alt=\"\" style=\"height: 400px;\"/>\n",
    "<img src=\"https://www.mrlc.gov/downloadfile.php?file=NLCD_Colour_Classification_Update.jpg\" alt=\"\" style=\"height: 400px;\"/>\n",
    "\n",
    "Ref:\n",
    "- https://www.mrlc.gov/\n",
    "- https://www.mrlc.gov/nlcd06_leg.php\n",
    "- https://landcover.usgs.gov/pdf/anderson.pdf\n",
    "\n",
    "#### Calculate Land Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/rasterstats/main.py:161: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(fsrc.array.dtype, float) and \\\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-870fd6ebda2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mbins_land_cover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbins_land_cover\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LAND_COVER\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbins_land_cover\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GEOMETRY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mland_cover_for_geometry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-870fd6ebda2f>\u001b[0m in \u001b[0;36mland_cover_for_geometry\u001b[0;34m(geometry)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mland_cover_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnodata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcategorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/rasterstats/main.py\u001b[0m in \u001b[0;36mzonal_stats\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0monly\u001b[0m \u001b[0mdifference\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzonal_stats\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     return a list rather than a generator.\"\"\"\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_zonal_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/rasterstats/main.py\u001b[0m in \u001b[0;36mgen_zonal_stats\u001b[0;34m(vectors, raster, layer, band, nodata, affine, stats, all_touched, categorical, category_map, add_stats, zone_func, raster_out, prefix, geojson_out, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mzone_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# nothing here, fill with None and move on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mfeature_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36mcompressed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3759\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnomask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "land_cover_path = \"/data/188-million-us-wildfires/src/nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img\"\n",
    "\n",
    "project = functools.partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj(init='EPSG:4326'),\n",
    "    pyproj.Proj('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs'))\n",
    "\n",
    "def land_cover_for_geometry(geometry):\n",
    "    stats = zonal_stats(\n",
    "        [shapely.geometry.mapping(transform(project, geometry))],\n",
    "        land_cover_path,\n",
    "        nodata=-999,\n",
    "        categorical=True\n",
    "    )\n",
    "    return stats[0]\n",
    "\n",
    "bins_land_cover = bins.copy()\n",
    "bins_land_cover[\"LAND_COVER\"] = bins_land_cover[\"GEOMETRY\"].apply(land_cover_for_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_land_cover.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins_land_cover['GEOM'] = bins_land_cover['GEOMETRY'].apply(lambda x: WKTElement(x.wkt))\n",
    "bins_land_cover.drop('GEOMETRY', 1, inplace=True, errors='ignore')\n",
    "\n",
    "bins_land_cover.to_sql(\n",
    "    'bins_land_cover_0',\n",
    "    engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype={\n",
    "        'H3': String(),\n",
    "        'LAND_COVER': JSON(),\n",
    "        'GEOM': Geometry('POLYGON')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revive [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:02:45.467766Z",
     "start_time": "2018-02-08T18:02:44.658901Z"
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins_land_cover = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT * FROM bins_land_cover_0;\",\n",
    "    engine,\n",
    "    geom_col='GEOM'\n",
    ")\n",
    "bins_land_cover.rename(mapper={'GEOM': 'GEOMETRY'}, axis=1, inplace=True)\n",
    "\n",
    "print(bins_land_cover.info())\n",
    "bins_land_cover.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Primary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_land_cover(land_cover):\n",
    "    return max(land_cover, key=land_cover.get)\n",
    "\n",
    "bins_land_cover[\"PRIMARY_LAND_COVER\"] = bins_land_cover[\"LAND_COVER\"].apply(primary_land_cover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorize Primary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_cover_categories = {\n",
    "    \"water\": [11, 12],\n",
    "    \"developed\": [21, 22, 23, 24],\n",
    "    \"barren\": [31],\n",
    "    \"forest\": [41, 42, 43],\n",
    "    \"shrubland\": [51, 52],\n",
    "    \"herbaceous\": [71, 72, 73, 74],\n",
    "    \"planted_cultivated\": [81, 82],\n",
    "    \"wetlands\": [90, 95]\n",
    "}\n",
    "\n",
    "land_cover_categories_map = {}\n",
    "for category, values in land_cover_categories.items():\n",
    "    for value in values:\n",
    "        land_cover_categories_map[value] = category\n",
    "\n",
    "def land_cover_category_for_class(land_cover_class):\n",
    "    return land_cover_categories_map[int(land_cover_class)] if int(land_cover_class) in land_cover_categories_map else 'unknown'\n",
    "\n",
    "bins_land_cover[\"PRIMARY_LAND_COVER_CATEGORY\"] = bins_land_cover[\"PRIMARY_LAND_COVER\"].apply(land_cover_category_for_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key, dataframe in fires_df.items():\n",
    "    label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    bins_land_cover.loc[:,'PRIMARY_LAND_COVER_CODE'] = label_encoder.fit_transform(bins_land_cover['PRIMARY_LAND_COVER'])\n",
    "    label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    bins_land_cover.loc[:,'PRIMARY_LAND_COVER_CATEGORY_CODE'] = label_encoder.fit_transform(bins_land_cover['PRIMARY_LAND_COVER_CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_land_cover.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "if 'GEOMETRY' in bins:\n",
    "    bins_land_cover['GEOM'] = bins['GEOMETRY'].apply(lambda x: WKTElement(x.wkt))\n",
    "    bins_land_cover.drop('GEOMETRY', 1, inplace=True, errors='ignore')\n",
    "\n",
    "bins_land_cover.to_sql(\n",
    "    'bins_land_cover_1',\n",
    "    engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype={\n",
    "        'H3': String(),\n",
    "        'LAND_COVER': JSON(),\n",
    "        'GEOM': Geometry('POLYGON')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revive [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:48:24.187414Z",
     "start_time": "2018-02-08T21:48:22.456874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 27099 entries, 0 to 27098\n",
      "Data columns (total 7 columns):\n",
      "H3                                  27099 non-null object\n",
      "LAND_COVER                          27099 non-null object\n",
      "PRIMARY_LAND_COVER                  27099 non-null object\n",
      "PRIMARY_LAND_COVER_CATEGORY         27099 non-null object\n",
      "PRIMARY_LAND_COVER_CODE             27099 non-null int64\n",
      "PRIMARY_LAND_COVER_CATEGORY_CODE    27099 non-null int64\n",
      "GEOMETRY                            27099 non-null object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H3</th>\n",
       "      <th>LAND_COVER</th>\n",
       "      <th>PRIMARY_LAND_COVER</th>\n",
       "      <th>PRIMARY_LAND_COVER_CATEGORY</th>\n",
       "      <th>PRIMARY_LAND_COVER_CODE</th>\n",
       "      <th>PRIMARY_LAND_COVER_CATEGORY_CODE</th>\n",
       "      <th>GEOMETRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>85260277fffffff</td>\n",
       "      <td>{'11': 6119, '21': 6632, '22': 383, '23': 6, '...</td>\n",
       "      <td>71</td>\n",
       "      <td>herbaceous</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((261.462245025 43.190780115, 261.5374...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15697</th>\n",
       "      <td>85299257fffffff</td>\n",
       "      <td>{'11': 86, '21': 428, '22': 8, '31': 22777, '4...</td>\n",
       "      <td>42</td>\n",
       "      <td>forest</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((247.258023306 38.838492619, 247.3209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>85268613fffffff</td>\n",
       "      <td>{'21': 10, '31': 10207, '52': 287214, '71': 291}</td>\n",
       "      <td>52</td>\n",
       "      <td>shrubland</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((268.848372878 36.465029202, 268.9202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11279</th>\n",
       "      <td>85280287fffffff</td>\n",
       "      <td>{'11': 3710, '21': 5582, '22': 731, '23': 249,...</td>\n",
       "      <td>42</td>\n",
       "      <td>forest</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((238.403109343 41.097920895, 238.4583...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26522</th>\n",
       "      <td>8548d9dbfffffff</td>\n",
       "      <td>{'11': 95, '21': 2348, '22': 456, '23': 16, '3...</td>\n",
       "      <td>52</td>\n",
       "      <td>shrubland</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((255.30877525 35.944773182, 255.37592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10060</th>\n",
       "      <td>85278047fffffff</td>\n",
       "      <td>{'11': 75715, '31': 10, '42': 8990, '52': 4362...</td>\n",
       "      <td>71</td>\n",
       "      <td>herbaceous</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((253.314476189 47.030568182, 253.2512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18290</th>\n",
       "      <td>852a918bfffffff</td>\n",
       "      <td>{'11': 4040, '21': 18659, '22': 9436, '23': 52...</td>\n",
       "      <td>82</td>\n",
       "      <td>planted_cultivated</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((276.390781179 41.325993793, 276.2972...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23688</th>\n",
       "      <td>8544de5bfffffff</td>\n",
       "      <td>{'11': 901, '21': 12373, '22': 3140, '23': 491...</td>\n",
       "      <td>82</td>\n",
       "      <td>planted_cultivated</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((274.30443852 31.501606001, 274.37273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>8526928bfffffff</td>\n",
       "      <td>{'21': 1836, '22': 1192, '23': 552, '24': 232,...</td>\n",
       "      <td>52</td>\n",
       "      <td>shrubland</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((249.002932794 41.089740611, 249.0688...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24559</th>\n",
       "      <td>8544f5b7fffffff</td>\n",
       "      <td>{'11': 672, '21': 12391, '22': 3368, '23': 857...</td>\n",
       "      <td>42</td>\n",
       "      <td>forest</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((260.517312685 28.315416512, 260.5825...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    H3                                         LAND_COVER  \\\n",
       "405    85260277fffffff  {'11': 6119, '21': 6632, '22': 383, '23': 6, '...   \n",
       "15697  85299257fffffff  {'11': 86, '21': 428, '22': 8, '31': 22777, '4...   \n",
       "3051   85268613fffffff   {'21': 10, '31': 10207, '52': 287214, '71': 291}   \n",
       "11279  85280287fffffff  {'11': 3710, '21': 5582, '22': 731, '23': 249,...   \n",
       "26522  8548d9dbfffffff  {'11': 95, '21': 2348, '22': 456, '23': 16, '3...   \n",
       "10060  85278047fffffff  {'11': 75715, '31': 10, '42': 8990, '52': 4362...   \n",
       "18290  852a918bfffffff  {'11': 4040, '21': 18659, '22': 9436, '23': 52...   \n",
       "23688  8544de5bfffffff  {'11': 901, '21': 12373, '22': 3140, '23': 491...   \n",
       "5006   8526928bfffffff  {'21': 1836, '22': 1192, '23': 552, '24': 232,...   \n",
       "24559  8544f5b7fffffff  {'11': 672, '21': 12391, '22': 3368, '23': 857...   \n",
       "\n",
       "      PRIMARY_LAND_COVER PRIMARY_LAND_COVER_CATEGORY  PRIMARY_LAND_COVER_CODE  \\\n",
       "405                   71                  herbaceous                       11   \n",
       "15697                 42                      forest                        8   \n",
       "3051                  52                   shrubland                       10   \n",
       "11279                 42                      forest                        8   \n",
       "26522                 52                   shrubland                       10   \n",
       "10060                 71                  herbaceous                       11   \n",
       "18290                 82          planted_cultivated                       13   \n",
       "23688                 82          planted_cultivated                       13   \n",
       "5006                  52                   shrubland                       10   \n",
       "24559                 42                      forest                        8   \n",
       "\n",
       "       PRIMARY_LAND_COVER_CATEGORY_CODE  \\\n",
       "405                                   3   \n",
       "15697                                 2   \n",
       "3051                                  5   \n",
       "11279                                 2   \n",
       "26522                                 5   \n",
       "10060                                 3   \n",
       "18290                                 4   \n",
       "23688                                 4   \n",
       "5006                                  5   \n",
       "24559                                 2   \n",
       "\n",
       "                                                GEOMETRY  \n",
       "405    POLYGON ((261.462245025 43.190780115, 261.5374...  \n",
       "15697  POLYGON ((247.258023306 38.838492619, 247.3209...  \n",
       "3051   POLYGON ((268.848372878 36.465029202, 268.9202...  \n",
       "11279  POLYGON ((238.403109343 41.097920895, 238.4583...  \n",
       "26522  POLYGON ((255.30877525 35.944773182, 255.37592...  \n",
       "10060  POLYGON ((253.314476189 47.030568182, 253.2512...  \n",
       "18290  POLYGON ((276.390781179 41.325993793, 276.2972...  \n",
       "23688  POLYGON ((274.30443852 31.501606001, 274.37273...  \n",
       "5006   POLYGON ((249.002932794 41.089740611, 249.0688...  \n",
       "24559  POLYGON ((260.517312685 28.315416512, 260.5825...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(\"postgresql://data:data@postgis/data\")\n",
    "\n",
    "bins_land_cover = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT * FROM bins_land_cover_1;\",\n",
    "    engine,\n",
    "    geom_col='GEOM'\n",
    ")\n",
    "bins_land_cover.rename(mapper={'GEOM': 'GEOMETRY'}, axis=1, inplace=True)\n",
    "\n",
    "print(bins_land_cover.info())\n",
    "bins_land_cover.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge onto `fires_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:48:36.096829Z",
     "start_time": "2018-02-08T21:48:30.089842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>OWNER_CODE</th>\n",
       "      <th>OWNER_DESCR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_CODE</th>\n",
       "      <th>DISCOVERY_WEEK</th>\n",
       "      <th>CLIMATE_REGION</th>\n",
       "      <th>CLIMATE_REGION_CODE</th>\n",
       "      <th>H3</th>\n",
       "      <th>LAND_COVER</th>\n",
       "      <th>PRIMARY_LAND_COVER</th>\n",
       "      <th>PRIMARY_LAND_COVER_CATEGORY</th>\n",
       "      <th>PRIMARY_LAND_COVER_CODE</th>\n",
       "      <th>PRIMARY_LAND_COVER_CATEGORY_CODE</th>\n",
       "      <th>GEOMETRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655464</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>250</td>\n",
       "      <td>30.3800</td>\n",
       "      <td>-86.2400</td>\n",
       "      <td>FL</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>southeast</td>\n",
       "      <td>5</td>\n",
       "      <td>8544e507fffffff</td>\n",
       "      <td>{'0': 49303, '11': 125855, '21': 10277, '22': ...</td>\n",
       "      <td>11</td>\n",
       "      <td>water</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>POLYGON ((272.490508228 34.335473341, 272.5608...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445404</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BIA</td>\n",
       "      <td>142</td>\n",
       "      <td>37.2014</td>\n",
       "      <td>-107.9008</td>\n",
       "      <td>CO</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>southwest</td>\n",
       "      <td>6</td>\n",
       "      <td>85269943fffffff</td>\n",
       "      <td>{'11': 5422, '21': 6424, '22': 6118, '23': 181...</td>\n",
       "      <td>42</td>\n",
       "      <td>forest</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((250.777522745 37.169224687, 250.8423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32476</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BIA</td>\n",
       "      <td>279</td>\n",
       "      <td>44.7725</td>\n",
       "      <td>-101.8524</td>\n",
       "      <td>SD</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>northern_rockies</td>\n",
       "      <td>1</td>\n",
       "      <td>85278937fffffff</td>\n",
       "      <td>{'11': 4216, '21': 399, '22': 198, '31': 298, ...</td>\n",
       "      <td>71</td>\n",
       "      <td>herbaceous</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((256.323987879 44.469068488, 256.3976...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811601</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>12</td>\n",
       "      <td>35.3500</td>\n",
       "      <td>-82.5833</td>\n",
       "      <td>NC</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>southeast</td>\n",
       "      <td>5</td>\n",
       "      <td>8544dd2bfffffff</td>\n",
       "      <td>{'11': 278, '21': 22381, '22': 3803, '23': 114...</td>\n",
       "      <td>41</td>\n",
       "      <td>forest</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((275.016204908 30.685343116, 275.0838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472040</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>8.0</td>\n",
       "      <td>PRIVATE</td>\n",
       "      <td>85</td>\n",
       "      <td>31.7209</td>\n",
       "      <td>-81.9151</td>\n",
       "      <td>GA</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>southeast</td>\n",
       "      <td>5</td>\n",
       "      <td>8544f337fffffff</td>\n",
       "      <td>{'11': 3998, '21': 30630, '22': 15270, '23': 4...</td>\n",
       "      <td>90</td>\n",
       "      <td>wetlands</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((276.712442226 29.697751453, 276.7790...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STAT_CAUSE_CODE STAT_CAUSE_DESCR  OWNER_CODE            OWNER_DESCR  \\\n",
       "655464              3.0          Smoking        14.0  MISSING/NOT SPECIFIED   \n",
       "445404              1.0        Lightning         2.0                    BIA   \n",
       "32476               5.0   Debris Burning         2.0                    BIA   \n",
       "811601              5.0   Debris Burning        14.0  MISSING/NOT SPECIFIED   \n",
       "472040              5.0   Debris Burning         8.0                PRIVATE   \n",
       "\n",
       "        DISCOVERY_DOY  LATITUDE  LONGITUDE STATE  STATE_CODE  DISCOVERY_WEEK  \\\n",
       "655464            250   30.3800   -86.2400    FL           8              36   \n",
       "445404            142   37.2014  -107.9008    CO           4              21   \n",
       "32476             279   44.7725  -101.8524    SD          39              40   \n",
       "811601             12   35.3500   -82.5833    NC          25               2   \n",
       "472040             85   31.7209   -81.9151    GA           9              13   \n",
       "\n",
       "          CLIMATE_REGION  CLIMATE_REGION_CODE               H3  \\\n",
       "655464         southeast                    5  8544e507fffffff   \n",
       "445404         southwest                    6  85269943fffffff   \n",
       "32476   northern_rockies                    1  85278937fffffff   \n",
       "811601         southeast                    5  8544dd2bfffffff   \n",
       "472040         southeast                    5  8544f337fffffff   \n",
       "\n",
       "                                               LAND_COVER PRIMARY_LAND_COVER  \\\n",
       "655464  {'0': 49303, '11': 125855, '21': 10277, '22': ...                 11   \n",
       "445404  {'11': 5422, '21': 6424, '22': 6118, '23': 181...                 42   \n",
       "32476   {'11': 4216, '21': 399, '22': 198, '31': 298, ...                 71   \n",
       "811601  {'11': 278, '21': 22381, '22': 3803, '23': 114...                 41   \n",
       "472040  {'11': 3998, '21': 30630, '22': 15270, '23': 4...                 90   \n",
       "\n",
       "       PRIMARY_LAND_COVER_CATEGORY  PRIMARY_LAND_COVER_CODE  \\\n",
       "655464                       water                        1   \n",
       "445404                      forest                        8   \n",
       "32476                   herbaceous                       11   \n",
       "811601                      forest                        7   \n",
       "472040                    wetlands                       14   \n",
       "\n",
       "        PRIMARY_LAND_COVER_CATEGORY_CODE  \\\n",
       "655464                                 7   \n",
       "445404                                 2   \n",
       "32476                                  3   \n",
       "811601                                 2   \n",
       "472040                                 8   \n",
       "\n",
       "                                                 GEOMETRY  \n",
       "655464  POLYGON ((272.490508228 34.335473341, 272.5608...  \n",
       "445404  POLYGON ((250.777522745 37.169224687, 250.8423...  \n",
       "32476   POLYGON ((256.323987879 44.469068488, 256.3976...  \n",
       "811601  POLYGON ((275.016204908 30.685343116, 275.0838...  \n",
       "472040  POLYGON ((276.712442226 29.697751453, 276.7790...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>OWNER_CODE</th>\n",
       "      <th>OWNER_DESCR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_CODE</th>\n",
       "      <th>DISCOVERY_WEEK</th>\n",
       "      <th>CLIMATE_REGION</th>\n",
       "      <th>CLIMATE_REGION_CODE</th>\n",
       "      <th>H3</th>\n",
       "      <th>LAND_COVER</th>\n",
       "      <th>PRIMARY_LAND_COVER</th>\n",
       "      <th>PRIMARY_LAND_COVER_CATEGORY</th>\n",
       "      <th>PRIMARY_LAND_COVER_CODE</th>\n",
       "      <th>PRIMARY_LAND_COVER_CATEGORY_CODE</th>\n",
       "      <th>GEOMETRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166753</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>113</td>\n",
       "      <td>48.642663</td>\n",
       "      <td>-96.478506</td>\n",
       "      <td>MN</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>south</td>\n",
       "      <td>4</td>\n",
       "      <td>85271d0bfffffff</td>\n",
       "      <td>{'11': 3061, '21': 11096, '22': 1874, '23': 95...</td>\n",
       "      <td>82</td>\n",
       "      <td>planted_cultivated</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((273.22646599 45.681986058, 273.12835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179460</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Equipment Use</td>\n",
       "      <td>7.0</td>\n",
       "      <td>STATE</td>\n",
       "      <td>133</td>\n",
       "      <td>36.386667</td>\n",
       "      <td>-88.958340</td>\n",
       "      <td>TN</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>ohio_valley</td>\n",
       "      <td>3</td>\n",
       "      <td>85264e0ffffffff</td>\n",
       "      <td>{'11': 1921, '21': 19979, '22': 5690, '23': 20...</td>\n",
       "      <td>82</td>\n",
       "      <td>planted_cultivated</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((266.18867902 38.539520251, 266.26170...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22821</th>\n",
       "      <td>11.0</td>\n",
       "      <td>Powerline</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>102</td>\n",
       "      <td>45.930333</td>\n",
       "      <td>-91.446143</td>\n",
       "      <td>WI</td>\n",
       "      <td>46</td>\n",
       "      <td>15</td>\n",
       "      <td>south</td>\n",
       "      <td>4</td>\n",
       "      <td>852750c7fffffff</td>\n",
       "      <td>{'11': 51503, '21': 23603, '22': 3054, '23': 1...</td>\n",
       "      <td>41</td>\n",
       "      <td>forest</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((267.701561111 45.333562832, 267.6133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103353</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Campfire</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>93</td>\n",
       "      <td>32.981100</td>\n",
       "      <td>-86.034240</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>southeast</td>\n",
       "      <td>5</td>\n",
       "      <td>8544e853fffffff</td>\n",
       "      <td>{'11': 2441, '21': 22756, '22': 5318, '23': 26...</td>\n",
       "      <td>41</td>\n",
       "      <td>forest</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((275.197562928 33.684264834, 275.2670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106693</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>14.0</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>219</td>\n",
       "      <td>33.101111</td>\n",
       "      <td>-116.433889</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>west</td>\n",
       "      <td>8</td>\n",
       "      <td>8529a687fffffff</td>\n",
       "      <td>{'21': 5961, '22': 1225, '23': 45, '31': 53853...</td>\n",
       "      <td>52</td>\n",
       "      <td>shrubland</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((244.346195637 33.18164638, 244.40340...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STAT_CAUSE_CODE STAT_CAUSE_DESCR  OWNER_CODE            OWNER_DESCR  \\\n",
       "166753              5.0   Debris Burning        14.0  MISSING/NOT SPECIFIED   \n",
       "179460              2.0    Equipment Use         7.0                  STATE   \n",
       "22821              11.0        Powerline        14.0  MISSING/NOT SPECIFIED   \n",
       "103353              4.0         Campfire        14.0  MISSING/NOT SPECIFIED   \n",
       "106693              1.0        Lightning        14.0  MISSING/NOT SPECIFIED   \n",
       "\n",
       "        DISCOVERY_DOY   LATITUDE   LONGITUDE STATE  STATE_CODE  \\\n",
       "166753            113  48.642663  -96.478506    MN          21   \n",
       "179460            133  36.386667  -88.958340    TN          40   \n",
       "22821             102  45.930333  -91.446143    WI          46   \n",
       "103353             93  32.981100  -86.034240    AL           0   \n",
       "106693            219  33.101111 -116.433889    CA           3   \n",
       "\n",
       "        DISCOVERY_WEEK CLIMATE_REGION  CLIMATE_REGION_CODE               H3  \\\n",
       "166753              17          south                    4  85271d0bfffffff   \n",
       "179460              20    ohio_valley                    3  85264e0ffffffff   \n",
       "22821               15          south                    4  852750c7fffffff   \n",
       "103353              14      southeast                    5  8544e853fffffff   \n",
       "106693              32           west                    8  8529a687fffffff   \n",
       "\n",
       "                                               LAND_COVER PRIMARY_LAND_COVER  \\\n",
       "166753  {'11': 3061, '21': 11096, '22': 1874, '23': 95...                 82   \n",
       "179460  {'11': 1921, '21': 19979, '22': 5690, '23': 20...                 82   \n",
       "22821   {'11': 51503, '21': 23603, '22': 3054, '23': 1...                 41   \n",
       "103353  {'11': 2441, '21': 22756, '22': 5318, '23': 26...                 41   \n",
       "106693  {'21': 5961, '22': 1225, '23': 45, '31': 53853...                 52   \n",
       "\n",
       "       PRIMARY_LAND_COVER_CATEGORY  PRIMARY_LAND_COVER_CODE  \\\n",
       "166753          planted_cultivated                       13   \n",
       "179460          planted_cultivated                       13   \n",
       "22821                       forest                        7   \n",
       "103353                      forest                        7   \n",
       "106693                   shrubland                       10   \n",
       "\n",
       "        PRIMARY_LAND_COVER_CATEGORY_CODE  \\\n",
       "166753                                 4   \n",
       "179460                                 4   \n",
       "22821                                  2   \n",
       "103353                                 2   \n",
       "106693                                 5   \n",
       "\n",
       "                                                 GEOMETRY  \n",
       "166753  POLYGON ((273.22646599 45.681986058, 273.12835...  \n",
       "179460  POLYGON ((266.18867902 38.539520251, 266.26170...  \n",
       "22821   POLYGON ((267.701561111 45.333562832, 267.6133...  \n",
       "103353  POLYGON ((275.197562928 33.684264834, 275.2670...  \n",
       "106693  POLYGON ((244.346195637 33.18164638, 244.40340...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, dataframe in fires_df.items():\n",
    "    fires_df[key] = dataframe.merge(bins_land_cover, on=\"H3\", how=\"left\", copy=False)\n",
    "    display(fires_df[key].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data: Population Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Population Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T22:01:42.486895Z",
     "start_time": "2018-02-08T22:00:22.389923Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': -inf, 'median': 0.13730229437351227}\n",
      "{'mean': 15.332184462550028, 'median': 7.531429290771484}\n",
      "{'mean': -inf, 'median': 15.33297061920166}\n",
      "{'mean': -inf, 'median': 4.106518745422363}\n",
      "{'mean': -inf, 'median': 0.1000959724187851}\n",
      "{'mean': -inf, 'median': 1.1422085762023926}\n",
      "{'mean': 7.16886258378328, 'median': 0.0955229178071022}\n",
      "{'mean': -inf, 'median': 0.23370881378650665}\n",
      "{'mean': -inf, 'median': -3.4028234663852886e+38}\n",
      "{'mean': -inf, 'median': 24.771141052246094}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/rasterstats/main.py:161: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(fsrc.array.dtype, float) and \\\n"
     ]
    }
   ],
   "source": [
    "census_population_path = \"/data/188-million-us-wildfires/src/usgrid_data_2010/uspop10.tif\"\n",
    "\n",
    "def mean_population_density_for_geometry(geometry):\n",
    "    \n",
    "    stats = zonal_stats(\n",
    "        [shapely.geometry.mapping(geometry)],\n",
    "        census_population_path,\n",
    "        nodata=0,\n",
    "        stats=[\"mean\", \"median\"]\n",
    "    )\n",
    "    print(stats[0])\n",
    "    return stats[0][\"mean\"]\n",
    "\n",
    "bins_population_density = bins.copy()\n",
    "bins_population_density[\"MEAN_POPULATION_DENSITY\"] = \\\n",
    "    bins_population_density[\"GEOMETRY\"].sample(10).apply(mean_population_density_for_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:12:17.273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 27099 entries, 0 to 27098\n",
      "Data columns (total 3 columns):\n",
      "H3                         27099 non-null object\n",
      "GEOMETRY                   27099 non-null object\n",
      "MEAN_POPULATION_DENSITY    27099 non-null float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 635.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bins_population_density.info())\n",
    "bins_population_density.MEAN_POPULATION_DENSITY.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_population_density[\"MEAN_POPULATION_DENSITY_CATEGORY\"] = \\\n",
    "    pd.cut(bins_population_density[\"MEAN_POPULATION_DENSITY_CATEGORY\"], 5, labels=False)\n",
    "    \n",
    "bins_population_density.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data [1]\n",
    "\n",
    "Now that we have engineered some new data, let's revisit some of our exploratory techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:20:03.913Z"
    }
   },
   "outputs": [],
   "source": [
    "cause_by_region = fires_df[\"train\"].groupby(['CLIMATE_REGION', 'STAT_CAUSE_DESCR'])\\\n",
    "    .size()\\\n",
    "    .unstack()\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    cause_by_region,\n",
    "    cbar_kws={'shrink':.9 }, \n",
    "    annot=False,\n",
    "    cmap='inferno_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "counts_by_land_cover = fires_df[\"train\"].groupby('PRIMARY_LAND_COVER')\\\n",
    "    .size().sort_values()\n",
    "counts_by_land_cover = counts_by_land_cover.sort_values()\n",
    "#counts_by_land_cover_pcts = counts_by_land_cover.apply(lambda x: 100 * x / float(counts_by_land_cover.sum()))\n",
    "\n",
    "display(counts_by_land_cover)\n",
    "\n",
    "ax = sns.barplot(counts_by_land_cover.index, counts_by_land_cover.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , ax = plt.subplots(figsize =(10, 10))\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "colormap = quant_colormap.mpl_colormap\n",
    "\n",
    "crosstab = pd.crosstab(\n",
    "    fires_df[\"train\"].PRIMARY_LAND_COVER,\n",
    "    columns=fires_df[\"train\"].STAT_CAUSE_DESCR,\n",
    "    normalize='index',\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    crosstab, \n",
    "    square=True,\n",
    "    cmap = colormap,\n",
    "    cbar_kws={'shrink':.9 }, \n",
    "    ax=ax,\n",
    "    annot=True, \n",
    "    annot_kws={'fontsize':9 }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , ax = plt.subplots(figsize=(10, 10))\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "colormap = quant_colormap.mpl_colormap\n",
    "\n",
    "crosstab = pd.crosstab(\n",
    "    fires_df[\"train\"].PRIMARY_LAND_COVER_CATEGORY,\n",
    "    columns=fires_df[\"train\"].STAT_CAUSE_DESCR,\n",
    "    normalize='index',\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    crosstab, \n",
    "    square=True,\n",
    "    cmap = colormap,\n",
    "    cbar_kws={'shrink':.9 }, \n",
    "    ax=ax,\n",
    "    annot=True, \n",
    "    annot_kws={'fontsize':9 }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Coorelation\n",
    "\n",
    "Ref: https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(10, 10))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    #colormap = quant_colormap.mpl_colormap\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(method='pearson'), \n",
    "        square=True,\n",
    "        cmap = colormap,\n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,\n",
    "        vmax=1.0,\n",
    "        linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "display(fires_df[\"train\"].corr(method='pearson').sort_values([\"STAT_CAUSE_CODE\"], ascending=False))\n",
    "correlation_heatmap(fires_df[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fires_df[\"train\"].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop text category fields that duplicate numerical category fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_train_df = train_df.drop([\n",
    "#     'STAT_CAUSE_DESCR',\n",
    "#     'OWNER_DESCR',\n",
    "#     'STATE',\n",
    "#     'CLIMATE_REGION',\n",
    "# ], axis=1)\n",
    "# clean_train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:04:42.670391Z",
     "start_time": "2018-02-08T18:04:42.224890Z"
    }
   },
   "outputs": [],
   "source": [
    "if not dummies:\n",
    "    dummies = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:16:14.807934Z",
     "start_time": "2018-02-08T18:16:11.032596Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, dataframe in fires_df.items():\n",
    "    dummies[\"{}_STATES\".format(key)] = pd.get_dummies(fires_df[\"train\"].STATE, prefix='IN_STATE')\n",
    "    drop_cols = [c for c in fires_df[key].columns if c[:8] == \"IN_STATE\"]\n",
    "    fires_df[key] = fires_df[key].drop(drop_cols, 1)\n",
    "    fires_df[key] = fires_df[key].join(dummies[\"{}_STATES\".format(key)])\n",
    "    display(fires_df[key].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Owner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:46:20.463349Z",
     "start_time": "2018-02-08T18:46:17.802487Z"
    }
   },
   "outputs": [],
   "source": [
    "for key, dataframe in fires_df.items():\n",
    "    dummies[\"{}_OWNERS\".format(key)] = pd.get_dummies(fires_df[\"train\"].OWNER_CODE, prefix='OWNED_BY')\n",
    "    drop_cols = [c for c in fires_df[key].columns if c[:8] == \"OWNED_BY\"]\n",
    "    fires_df[key] = fires_df[key].drop(drop_cols, 1)\n",
    "    fires_df[key] = fires_df[key].join(dummies[\"{}_OWNERS\".format(key)])\n",
    "    display(fires_df[key].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Helper\n",
    "\n",
    "Ref:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:18:26.521506Z",
     "start_time": "2018-02-08T18:18:26.492180Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=quant_colormap.mpl_colormap):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(shrink=0.65)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh =  cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Actual label')\n",
    "    plt.ylabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Benchmark\n",
    "\n",
    "Before we train a model, let's establish a benchmark to work against. To do this, we will develop by hand, a very simple decision tree.\n",
    "\n",
    "The simplest benchmark that we can set is with a model that always evaluates to a single response. Our benchmark model will evaluate to `5.0`, the code representative of 'Debris Burning'.\n",
    "\n",
    "TODO: this benchmark should be improved once we have engineered some new data into our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:18:29.770877Z",
     "start_time": "2018-02-08T18:18:29.720546Z"
    }
   },
   "outputs": [],
   "source": [
    "source_fields = [\n",
    "    'OWNER_CODE',\n",
    "    'DISCOVERY_WEEK',\n",
    "    'STATE_CODE'\n",
    "]\n",
    "\n",
    "target_fields = [\n",
    "    'STAT_CAUSE_CODE'\n",
    "]\n",
    "\n",
    "def benchmark_model(df):\n",
    "    return pd.DataFrame(data = {'STAT_CAUSE_CODE_PREDICT':[5.0 for i in range(0, df.shape[0])]})\n",
    "\n",
    "prediction = benchmark_model(fires_df[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of our benchmark model. It should fall right around 23% - this is the rough proportion of fires caused by 'Debris Burning'.\n",
    "\n",
    "Ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score  http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T18:18:33.820497Z",
     "start_time": "2018-02-08T18:18:32.318620Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    ")\n",
    "\n",
    "print('Accuracy: {} \\n\\n'.format(accuracy))\n",
    "\n",
    "print(sklearn.metrics.classification_report(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    "))\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(fires_df[\"test\"][target_fields], prediction)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    stat_cause_mapping.values,\n",
    "    normalize=True,\n",
    "    title='Normalized confusion matrix'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) Classifier\n",
    "\n",
    "[TKTK]\n",
    "\n",
    "\n",
    "Ref:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier\n",
    "\n",
    "http://scikit-learn.org/stable/modules/sgd.html#classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "source_fields = [\n",
    "    'OWNER_CODE',\n",
    "    'DISCOVERY_WEEK',\n",
    "    'STATE_CODE'\n",
    "]\n",
    "\n",
    "target_fields = [\n",
    "    'STAT_CAUSE_CODE'\n",
    "]\n",
    "\n",
    "classifier = SGDClassifier(\n",
    "    loss=\"hinge\",\n",
    "    penalty=\"l2\",\n",
    "    max_iter=5,\n",
    "    tol=None,\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    fires_df[\"train\"][target_fields]['STAT_CAUSE_CODE']\n",
    ")\n",
    "\n",
    "prediction = classifier.predict(\n",
    "    fires_df[\"test\"][source_fields]\n",
    ")\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    ")\n",
    "\n",
    "print('Accuracy: {} \\n\\n'.format(accuracy))\n",
    "\n",
    "print(sklearn.metrics.classification_report(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    "))\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(fires_df[\"test\"][target_fields], prediction)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    stat_cause_mapping.values,\n",
    "    normalize=True,\n",
    "    title='Normalized confusion matrix'\n",
    ")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "Ref:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "source_fields = [\n",
    "    'OWNER_CODE',\n",
    "    'DISCOVERY_WEEK',\n",
    "    'STATE_CODE',\n",
    "#     'CLIMATE_REGION_CODE',\n",
    "#     'PRIMARY_LAND_COVER_CODE',\n",
    "]\n",
    "\n",
    "target_fields = [\n",
    "    'STAT_CAUSE_CODE'\n",
    "]\n",
    "\n",
    "classifier = DecisionTreeClassifier(\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    fires_df[\"train\"][target_fields]\n",
    ")\n",
    "\n",
    "prediction = classifier.predict(\n",
    "    fires_df[\"test\"][source_fields]\n",
    ")\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    ")\n",
    "\n",
    "print('Accuracy: {} \\n\\n'.format(accuracy))\n",
    "\n",
    "print(sklearn.metrics.classification_report(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    "))\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(fires_df[\"test\"][target_fields], prediction)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    stat_cause_mapping.values,\n",
    "    normalize=True,\n",
    "    title='Normalized confusion matrix'\n",
    ")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:49:19.371513Z",
     "start_time": "2018-02-08T21:49:19.336512Z"
    }
   },
   "outputs": [],
   "source": [
    "source_fields = [\n",
    "     'OWNER_CODE',\n",
    "     'DISCOVERY_DOY',\n",
    "     'STATE_CODE',\n",
    "    #'CLIMATE_REGION_CODE',\n",
    "     'PRIMARY_LAND_COVER_CODE_y',\n",
    "]\n",
    "#source_fields = source_fields + list(dummies['train_OWNERS'].columns)\n",
    "#source_fields = source_fields + list(dummies['train_STATES'].columns)\n",
    "\n",
    "\n",
    "target_fields = [\n",
    "    'STAT_CAUSE_CODE'\n",
    "]\n",
    "\n",
    "display(fires_df[\"train\"][source_fields].head(5))\n",
    "display(fires_df[\"train\"][target_fields].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:16:50.180Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:19:57.038Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgtrain = xgb.DMatrix(\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    label=fires_df[\"train\"][target_fields]\n",
    ")\n",
    "xgtrain.save_binary('/data/188-million-us-wildfires/xgtrain.buffer')\n",
    "\n",
    "xgtest = xgb.DMatrix(\n",
    "    fires_df[\"test\"][source_fields]\n",
    ")\n",
    "xgtest.save_binary('/data/188-million-us-wildfires/xgtest.buffer')\n",
    "\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'silent': 0,\n",
    "    'nthread': 4,\n",
    "    \n",
    "    'eta': 0.3,\n",
    "    'gamma': 0,\n",
    "    'max_depth': 45,\n",
    "    'min_child_weight': 1,\n",
    "    'max_delta_step': 0,\n",
    "    'subsample': 1,\n",
    "    \n",
    "    'objective':'multi:softmax',\n",
    "    'num_class': 12\n",
    "}\n",
    "\n",
    "num_round = 5\n",
    "bst = xgb.train(\n",
    "    param,\n",
    "    xgtrain,\n",
    "    num_round\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:10:19.729Z"
    }
   },
   "outputs": [],
   "source": [
    "train_prediction = bst.predict(xgtrain)\n",
    "accuracy = sklearn.metrics.accuracy_score(\n",
    "    fires_df[\"train\"][target_fields],\n",
    "    train_prediction\n",
    ")\n",
    "print('Train Accuracy: {} \\n\\n'.format(accuracy))\n",
    "print(sklearn.metrics.classification_report(\n",
    "    fires_df[\"train\"][target_fields],\n",
    "    train_prediction\n",
    "))\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(\n",
    "    fires_df[\"train\"][target_fields],\n",
    "    train_prediction\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    stat_cause_mapping.values,\n",
    "    normalize=True,\n",
    "    title='Normalized confusion matrix'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:10:20.588Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = bst.predict(xgtest)\n",
    "accuracy = sklearn.metrics.accuracy_score(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    ")\n",
    "print('Test Accuracy: {} \\n\\n'.format(accuracy))\n",
    "print(sklearn.metrics.classification_report(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    prediction\n",
    "))\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(fires_df[\"test\"][target_fields], prediction)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    stat_cause_mapping.values,\n",
    "    normalize=True,\n",
    "    title='Normalized confusion matrix'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-08T22:11:57.471Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "importance = bst.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "importance_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "importance_df['fscore'] = importance_df['fscore'] / importance_df['fscore'].sum()\n",
    "\n",
    "display(importance_df)\n",
    "\n",
    "plt.figure()\n",
    "importance_df.plot()\n",
    "importance_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier Redux\n",
    "\n",
    "Ref:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fields = [\n",
    "    'OWNER_CODE',\n",
    "#     'DISCOVERY_WEEK',\n",
    "    'STATE_CODE'\n",
    "]\n",
    "\n",
    "target_fields = [\n",
    "    'STAT_CAUSE_CODE'\n",
    "]\n",
    "\n",
    "cv_split = sklearn.model_selection.ShuffleSplit(\n",
    "    n_splits=10,\n",
    "    test_size=0.3,\n",
    "    train_size=0.6,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = DecisionTreeClassifier(\n",
    "    class_weight=None,\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    presort=False,\n",
    "    random_state=0,\n",
    "    splitter='best'\n",
    ")\n",
    "\n",
    "base_results = model_selection.cross_validate(\n",
    "    base_classifier,\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    fires_df[\"train\"][target_fields],\n",
    "    cv = cv_split\n",
    ")\n",
    "\n",
    "base_classifier.fit(\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    fires_df[\"train\"][target_fields],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_classifier.tree_.node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(base_classifier.get_params())\n",
    "print(\"Base Score: {}\".format(base_results['test_score'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = sklearn.tree.export_graphviz(\n",
    "    base_classifier,\n",
    "    out_file=None, \n",
    "    feature_names=source_fields,\n",
    "    class_names=True,\n",
    "    filled=True,\n",
    "    rounded = True\n",
    ")\n",
    "print(dot_data)\n",
    "graph = graphviz.Source(dot_data, engine='sfdp') \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Setup hyperparameter grid for decision tree. Using GridSearchCV, we will try various combinations of the parameters that we define.\n",
    "\n",
    "Ref:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"splitter\": [\"best\"],\n",
    "    \"max_depth\": [17, 19, 20, 22, 25],\n",
    "    \"min_samples_split\": [20, 40, 60],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "    \"min_weight_fraction_leaf\": [0],\n",
    "    \"max_features\": [None],\n",
    "    \"random_state\": [0],\n",
    "    \"max_leaf_nodes\": [None],\n",
    "    \"min_impurity_decrease\": [0.],\n",
    "    \"class_weight\": [None]\n",
    "}\n",
    "\n",
    "tuned_classifier = sklearn.model_selection.GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    hyperparameter_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cv_split\n",
    ")\n",
    "\n",
    "tuned_classifier.fit(\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    fires_df[\"train\"][target_fields]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(tuned_classifier.best_params_)\n",
    "print(\"Tuned Score: {}\".format(tuned_classifier.cv_results_['mean_test_score'][tuned_classifier.best_index_]))\n",
    "\n",
    "\n",
    "tuned_prediction = classifier.predict(\n",
    "    fires_df[\"test\"][source_fields]\n",
    ")\n",
    "\n",
    "tuned_accuracy = sklearn.metrics.accuracy_score(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    tuned_prediction\n",
    ")\n",
    "\n",
    "print('Accuracy: {} \\n\\n'.format(tuned_accuracy))\n",
    "\n",
    "print(sklearn.metrics.classification_report(\n",
    "    fires_df[\"test\"][target_fields],\n",
    "    tuned_prediction\n",
    "))\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(fires_df[\"test\"][target_fields], tuned_prediction)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    stat_cause_mapping.values,\n",
    "    normalize=True,\n",
    "    title='Normalized confusion matrix'\n",
    ")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Elimination\n",
    "\n",
    "[TKTK]\n",
    "\n",
    "Ref:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_classifier = sklearn.feature_selection.RFECV(\n",
    "    base_classifier,\n",
    "    step=1,\n",
    "    scoring='accuracy',\n",
    "    cv=cv_split\n",
    ")\n",
    "\n",
    "rfe_classifier.fit(\n",
    "    fires_df[\"train\"][source_fields],\n",
    "    fires_df[\"train\"][target_fields]\n",
    ")\n",
    "\n",
    "rfe_support_columns = fires_df[\"train\"][source_fields].columns.values[rfe_classifier.get_support()]\n",
    "\n",
    "rfe_results = model_selection.cross_validation(\n",
    "    base_classifier,\n",
    "    fires_df[\"train\"][rfe_support_columns],\n",
    "    fires_df[\"train\"][target_fields],\n",
    "    cv=cv_split\n",
    ")\n",
    "\n",
    "print(rfe_results)\n",
    "\n",
    "rfe_tuned_classifier = sklearn.model_selection.GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    hyperparameter_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cv_split\n",
    ")\n",
    "\n",
    "rfe_tuned_classifier.fit(\n",
    "    fires_df[\"train\"][rfe_support_columns],\n",
    "    fires_df[\"train\"][target_fields]\n",
    ")\n",
    "\n",
    "print(\"RFE + Tuned Score: {}\".format(rfe_tuned_classifier.cv_results_['mean_test_score'][classifier.best_index_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notify_time": "5",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
